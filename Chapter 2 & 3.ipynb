{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\path\\\\to\\\\corpus\\\\root'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c99c04e0b702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mCAT_PATTERN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'([\\w_\\s]+)/.*'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m corpus = CategorizedPlaintextCorpusReader(\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;34m'/path/to/corpus/root'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDOC_PATTERN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCAT_PATTERN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \"\"\"\n\u001b[0;32m    173\u001b[0m         \u001b[0mCategorizedCorpusReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[0mPlaintextCorpusReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_resolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, fileids, word_tokenizer, sent_tokenizer, para_block_reader, encoding)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mcorpus\u001b[0m \u001b[0minto\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \"\"\"\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mCorpusReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sent_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, fileids, encoding, tagset)\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzipentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPathPointer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CorpusReader: expected a string or a PathPointer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\path\\\\to\\\\corpus\\\\root'"
     ]
    }
   ],
   "source": [
    "# Example of type of corpus reader:\n",
    "\n",
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "\n",
    "DOC_PATTERN = r'(?!\\.)[\\w_\\s]+/[\\w\\s\\d\\-]+\\.txt'\n",
    "CAT_PATTERN = r'([\\w_\\s]+)/.*'\n",
    "\n",
    "corpus = CategorizedPlaintextCorpusReader(\n",
    "    '/path/to/corpus/root', DOC_PATTERN, cat_pattern=CAT_PATTERN\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "would operate on corpus:\n",
    "corpus\n",
    "├── LICENSE\n",
    "├── README\n",
    "└── Star Trek\n",
    "|   ├── Star Trek - Balance of Terror.txt\n",
    "|   ├── Star Trek - First Contact.txt\n",
    "|   ├── Star Trek - Generations.txt\n",
    "|   ├── Star Trek - Nemesis.txt\n",
    "|   ├── Star Trek - The Motion Picture.txt\n",
    "|   ├── Star Trek 2 - The Wrath of Khan.txt\n",
    "|   └── Star Trek.txt\n",
    "└── Star Wars\n",
    "|   ├── Star Wars Episode 1.txt\n",
    "|   ├── Star Wars Episode 2.txt\n",
    "|   ├── Star Wars Episode 3.txt\n",
    "|   ├── Star Wars Episode 4.txt\n",
    "|   ├── Star Wars Episode 5.txt\n",
    "|   ├── Star Wars Episode 6.txt\n",
    "|   └── Star Wars Episode 7.txt\n",
    "└── citation.bib\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The document pattern regular expression specifies \n",
    "documents as having paths under the corpus root \n",
    "such that there is one or more letters, digits, \n",
    "spaces, or underscores, followed by the / character, \n",
    "then one or more letters, digits, spaces, \n",
    "or hyphens followed by .txt. This will match \n",
    "documents such as Star Wars/Star Wars Episode 1.txt \n",
    "but not documents such as episode.txt. \n",
    "The categories pattern regular expression truncates \n",
    "the original regular expression with a capture \n",
    "group that indicates that a category is any directory \n",
    "name (e.g., Star Wars/anything.txt will capture \n",
    "Star Wars as the category).\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HTML documents, to keep all tags, use HTMLCorpusReader\n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "import codecs\n",
    "\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
    "\n",
    "class HTMLCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for raw HTML documents to enable preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, fileids=DOC_PATTERN, encoding='utf8',\n",
    "                 tags=TAGS, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining\n",
    "        arguments are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        # Initialize the NLTK corpus reader objects\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "\n",
    "        # Save the tags that we specifically want to extract.\n",
    "        self.tags = tags\n",
    "        \n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. Implemented similarly to\n",
    "        the NLTK ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "    \n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the complete text of an HTML document, closing the document\n",
    "        after we are done reading it and yielding it in a memory safe fashion.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                yield f.read()\n",
    "                \n",
    "    def sizes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tuples, the fileid and size on disk of the file.\n",
    "        This function is used to detect oddly large files in the corpus.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, getting every path and computing filesize\n",
    "        for path in self.abspaths(fileids):\n",
    "            yield os.path.getsize(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using database:\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "class SqliteCorpusReader(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self._cur = sqlite3.connect(path).cursor()\n",
    "\n",
    "    def ids(self):\n",
    "        \"\"\"\n",
    "        Returns the review ids, which enable joins to other\n",
    "        review metadata\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT reviewid FROM content\")\n",
    "        for idx in iter(self._cur.fetchone, None):\n",
    "            yield idx\n",
    "\n",
    "    def scores(self):\n",
    "        \"\"\"\n",
    "        Returns the review score, to be used as the target\n",
    "        for later supervised learning problems\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT score FROM reviews\")\n",
    "        for score in iter(self._cur.fetchone, None):\n",
    "            yield score\n",
    "\n",
    "    def texts(self):\n",
    "        \"\"\"\n",
    "        Returns the full review texts, to be preprocessed and\n",
    "        vectorized for supervised learning\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT content FROM content\")\n",
    "        for text in iter(self._cur.fetchone, None):\n",
    "            yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 3\n",
    "\n",
    "# for HTML documents, to keep all tags, use HTMLCorpusReader\n",
    "\n",
    "\"\"\"\n",
    "The html method iterates over each file and uses \n",
    "the summary method from the readability.Document \n",
    "class to remove any nontext content as well as \n",
    "script and stylistic tags. It also corrects any \n",
    "of the most commonly misused tags \n",
    "(e.g., <div> and <br>), only throwing an \n",
    "exception if the original HTML is found \n",
    "to be unparseable.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Note that the above method may generate \n",
    "warnings about the readability logger; \n",
    "you can adjust the level of verbosity \n",
    "according to your taste by adding\n",
    "\n",
    "import logging\n",
    "log = logging.getLogger(\"readability.readability\")\n",
    "log.setLevel('WARNING')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "import codecs\n",
    "\n",
    "from readability.readability import Unparseable\n",
    "from readability.readability import Document as Paper\n",
    "\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
    "\n",
    "import bs4\n",
    "import time\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "class HTMLCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for raw HTML documents to enable preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, fileids=DOC_PATTERN, encoding='utf8',\n",
    "                 tags=TAGS, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining\n",
    "        arguments are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        # Initialize the NLTK corpus reader objects\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "\n",
    "        # Save the tags that we specifically want to extract.\n",
    "        self.tags = tags\n",
    "        \n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. Implemented similarly to\n",
    "        the NLTK ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "    \n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the complete text of an HTML document, closing the document\n",
    "        after we are done reading it and yielding it in a memory safe fashion.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                yield f.read()\n",
    "                \n",
    "    def sizes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tuples, the fileid and size on disk of the file.\n",
    "        This function is used to detect oddly large files in the corpus.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, getting every path and computing filesize\n",
    "        for path in self.abspaths(fileids):\n",
    "            yield os.path.getsize(path)\n",
    "\n",
    "    # \n",
    "    def html(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the HTML content of each document, cleaning it using\n",
    "        the readability-lxml library.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            try:\n",
    "                yield Paper(doc).summary()\n",
    "            except Unparseable as e:\n",
    "                print(\"Could not parse HTML: {}\".format(e))\n",
    "                continue\n",
    "                \n",
    "\n",
    "    # separate cleaned html (using html() method) into paragraphs\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses BeautifulSoup to parse the paragraphs from the HTML.\n",
    "        \"\"\"\n",
    "        for html in self.html(fileids, categories):\n",
    "            soup = bs4.BeautifulSoup(html, 'lxml')\n",
    "            for element in soup.find_all(TAGS):\n",
    "                yield element.text\n",
    "            soup.decompose()\n",
    "            \n",
    "    # this gets sentences from paragraphs; there are multiply\n",
    "    # models that can be used\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built in sentence tokenizer to extract sentences from the\n",
    "        paragraphs. Note that this method uses BeautifulSoup to parse HTML.\n",
    "        \"\"\"\n",
    "\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in sent_tokenize(paragraph):\n",
    "                yield sentence\n",
    "                \n",
    "    # simlar to sentences, get words, with many other models available\n",
    "    # like TreebankWordTokenizer, WordPunctTokenize, \n",
    "    # PunktWordTokenizer, word_tokenize, RegexpTokenizer \n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built-in word tokenizer to extract tokens from sentences.\n",
    "        Note that this method uses BeautifulSoup to parse HTML content.\n",
    "        \"\"\"\n",
    "        for sentence in self.sents(fileids, categories):\n",
    "            for token in wordpunct_tokenize(sentence):\n",
    "                yield token\n",
    "        \n",
    "    # get parts-of-speech tagging in tuple, (tag, token)\n",
    "    # using paragraph and sentence, though using \n",
    "    # wordpunct_tokenize here instead of custom words() above\n",
    "    \n",
    "    # list of tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    def tokenize(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Segments, tokenizes, and tags a document in the corpus.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids=fileids):\n",
    "            yield [\n",
    "                pos_tag(wordpunct_tokenize(sent))\n",
    "                for sent in sent_tokenize(paragraph)\n",
    "            ]\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    We’ll keep a count of each paragraph, sentence, \n",
    "    and word, and we’ll also store each unique token \n",
    "    in our vocabulary. We then compute the number of \n",
    "    files and categories in our corpus, and return a \n",
    "    dictionary with a statistical summary of our \n",
    "    corpus—its total number of files and categories; \n",
    "    the total number of paragraph, sentences, and words;\n",
    "    the number of unique terms; the lexical diversity, \n",
    "    which is the ratio of unique terms to total words; \n",
    "    the average number of paragraphs per document; \n",
    "    the average number of sentences per paragraph; \n",
    "    and the total processing time:\n",
    "    \"\"\"\n",
    "    # this is a monitoring technique over time\n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and\n",
    "        returns a dictionary with a variety of metrics\n",
    "        concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        started = time.time()\n",
    "\n",
    "        # Structures to perform counting.\n",
    "        counts  = nltk.FreqDist()\n",
    "        tokens  = nltk.FreqDist()\n",
    "\n",
    "        # Perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.paras(fileids, categories):\n",
    "            counts['paras'] += 1\n",
    "\n",
    "            for sent in para:\n",
    "                counts['sents'] += 1\n",
    "\n",
    "                for word, tag in sent:\n",
    "                    counts['words'] += 1\n",
    "                    tokens[word] += 1\n",
    "\n",
    "        # Compute the number of files and categories in the corpus\n",
    "        n_fileids = len(self.resolve(fileids, categories) or self.fileids())\n",
    "        n_topics  = len(self.categories(self.resolve(fileids, categories)))\n",
    "\n",
    "        # Return data structure with information\n",
    "        return {\n",
    "            'files':  n_fileids,\n",
    "            'topics': n_topics,\n",
    "            'paras':  counts['paras'],\n",
    "            'sents':  counts['sents'],\n",
    "            'words':  counts['words'],\n",
    "            'vocab':  len(tokens),\n",
    "            'lexdiv': float(counts['words']) / float(len(tokens)),\n",
    "            'ppdoc':  float(counts['paras']) / float(n_fileids),\n",
    "            'sppar':  float(counts['sents']) / float(counts['paras']),\n",
    "            'secs':   time.time() - started,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# wrapper around HTMLCorpusReader, it creates stateful\n",
    "# tokenizatoin and part-of-speech tagging\n",
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    The preprocessor wraps an `HTMLCorpusReader` and performs tokenization\n",
    "    and part-of-speech tagging.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, target=None, **kwargs):\n",
    "        self.corpus = corpus\n",
    "        self.target = target\n",
    "\n",
    "    def fileids(self, fileids=None, categories=None):\n",
    "        fileids = self.corpus.resolve(fileids, categories)\n",
    "        if fileids:\n",
    "            return fileids\n",
    "        return self.corpus.fileids()\n",
    "\n",
    "    def abspath(self, fileid):\n",
    "        # Find the directory, relative to the corpus root.\n",
    "        parent = os.path.relpath(\n",
    "            os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root\n",
    "        )\n",
    "\n",
    "        # Compute the name parts to reconstruct\n",
    "        basename  = os.path.basename(fileid)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "\n",
    "        # Create the pickle file extension\n",
    "        basename  = name + '.pickle'\n",
    "\n",
    "        # Return the path to the file relative to the target.\n",
    "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
    "    \n",
    "    #given a raw document, will perform segmentation, \n",
    "    #tokenization, and part-of-speech tagging using \n",
    "    #the NLTK methods we explored in the previous \n",
    "    #section\n",
    "    def tokenize(self, fileid):\n",
    "        for paragraph in self.corpus.paras(fileids=fileid):\n",
    "            yield [\n",
    "                pos_tag(wordpunct_tokenize(sent))\n",
    "                for sent in sent_tokenize(paragraph)\n",
    "            ]\n",
    "            \n",
    "\n",
    "    # store compressed object on disk\n",
    "    def process(self, fileid):\n",
    "        \"\"\"\n",
    "        For a single file, checks the location on disk to ensure no errors,\n",
    "        uses +tokenize()+ to perform the preprocessing, and writes transformed\n",
    "        document as a pickle to target location.\n",
    "        \"\"\"\n",
    "        # Compute the outpath to write the file to.\n",
    "        target = self.abspath(fileid)\n",
    "        parent = os.path.dirname(target)\n",
    "\n",
    "        # Make sure the directory exists\n",
    "        if not os.path.exists(parent):\n",
    "            os.makedirs(parent)\n",
    "\n",
    "        # Make sure that the parent is a directory and not a file\n",
    "        if not os.path.isdir(parent):\n",
    "            raise ValueError(\n",
    "                \"Please supply a directory to write preprocessed data to.\"\n",
    "            )\n",
    "\n",
    "        # Create a data structure for the pickle\n",
    "        document = list(self.tokenize(fileid))\n",
    "\n",
    "        # Open and serialize the pickle to disk\n",
    "        with open(target, 'wb') as f:\n",
    "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Clean up the document\n",
    "        del document\n",
    "\n",
    "        # Return the target fileid\n",
    "        return target\n",
    "    \n",
    "    #Our preprocess() method will be called multiple \n",
    "    #times by the following transform() runner:\n",
    "    def transform(self, fileids=None, categories=None):\n",
    "        # Make the target directory if it doesn't already exist\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "\n",
    "        # Resolve the fileids to start processing\n",
    "        for fileid in self.fileids(fileids, categories):\n",
    "            yield self.process(fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To read our corpus, we require a PickledCorpusReader\n",
    "class that uses pickle.load() to quickly retrieve \n",
    "the Python structures from one document at a time.\n",
    "\n",
    " override the HTMLCorpusReader docs() method with \n",
    " one that knows to load documents from pickles:\n",
    "\"\"\"\n",
    "\n",
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "\n",
    "class PickledCorpusReader(HTMLCorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        # Load one pickled document into memory at a time.\n",
    "        for path in self.abspaths(fileids):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "                \n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for para in doc:\n",
    "                yield para\n",
    "                \n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        for para in self.paras(fileids, categories):\n",
    "            for sent in para:\n",
    "                yield sent\n",
    "    \n",
    "    #The first tagged() method returns the token \n",
    "    #and tag together, the second words() \n",
    "    #method returns only the token in question\n",
    "    def tagged(self, fileids=None, categories=None):\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for tagged_token in sent:\n",
    "                yield tagged_token\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        for tagged in self.tagged(fileids, categories):\n",
    "            yield tagged[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
